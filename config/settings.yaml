#  config/settings.yaml

zeroai:
  mode: "smart"              # local, smart, or cloud
  cost_optimization: true    # Minimize costs automatically
  tagline: "Zero Cost. Zero Cloud. Zero Limits."

model:
  name: "llama3.2:1b"
  temperature: 0.3
  max_tokens: 256
  base_url: "http://ollama:11434"

agents:
  max_concurrent: 1
  timeout: 120
  verbose: true

logging:
  level: "INFO"
  file: "logs/zeroai.log"
  format: "%(asctime)s - ZeroAI - %(levelname)s - %(message)s"

cloud:
  provider: "local"  # Options: local, openai, anthropic, azure, google
  supervisor: "anthropic"  # Claude as supervisor/reviewer
  claude_agent_enabled: true  # Enable/disable Claude as team member
  # Set API keys in environment variables:
  # OPENAI_API_KEY, ANTHROPIC_API_KEY, AZURE_OPENAI_API_KEY, GOOGLE_API_KEY

thunder:
  enabled: true                    # Enable/disable Thunder Compute
  auto_start: true                # Auto-start for complex tasks
  complexity_threshold: 7         # Complexity level (1-10) to trigger Thunder
  api_key_env: "THUNDER_API_KEY"  # Environment variable for API key
  instance_id_env: "THUNDER_INSTANCE_ID"  # Environment variable for instance ID

smart_routing:
  local_only_mode: false          # Force all tasks to local processing
  fallback_provider: "openai"     # Fallback when Thunder fails
  auto_shutdown_delay: 1800       # Auto-shutdown Thunder after 30 min idle

internal:
  persistent_crews:
    enabled: true                 # Enable 24/7 persistent crew operations
    auto_start: true             # Auto-start persistent crews on system boot
    max_queue_size: 100          # Maximum tasks in queue per crew
    idle_timeout: 3600           # Shutdown crew after 1 hour idle (0 = never)
    default_projects: ["zeroai", "testcorp"]  # Projects to start by default
  interactive_mode:
    enabled: true                # Enable real-time crew communication
    graceful_shutdown: true      # Allow graceful shutdown on Ctrl+C
    status_updates: true         # Show live status updates
  resource_management:
    memory_limit: "2GB"          # Memory limit per persistent crew
    cpu_limit: 2                 # CPU cores per crew
    cleanup_interval: 300        # Cleanup interval in seconds